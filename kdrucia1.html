<h1 id="assignment-1-image-processing">Assignment 1: Image Processing</h1>
<p>Kevin Druciak</p>
<p>kdrucia1</p>
<p>MSVC / Visual Studio Community 2017 / Windows 10</p>
<p>Late days used: 1</p>
<p>All output is located in the Assignment1 folder. </p>

<h3 id="addrandomnoise">addRandomNoise</h3>
<p>I simply generated a uniform random distribution in the range (-noise, noise). From then I sampled the random distribution for each channel of every pixel in the image.</p>
<h3 id="brighten">brighten</h3>
<p>Brightening the image is as simple as multiplying each of the channels by the brightening factor. </p>
<h3 id="luminance">luminance</h3>
<p>In order to make the image gray, I set the channels of every pixel to the average of its channels according to the ratio 30:11:59 (R:B:G).</p>
<h3 id="contrast">contrast</h3>
<p>Here I first calculate the average luminance of the entire image.  I use that average to scale the image accordingly with the contrast factor.</p>
<p><img src=".\Assignment1\kdrucia1_HTML\contrast.gif" alt=""></p>
<h3 id="saturate">saturate</h3>
<p>I calculate the saturation of the image much in the same way that I calculate contrast. Instead of finding the average luminance for the entire image, I simply find the luminance for the pixel being operated on. </p>
<p><img src=".\Assignment1\kdrucia1_HTML\saturate.gif" alt=""></p>
<h3 id="crop">crop</h3>
<p>I simply iterate through the pixels in the range of the given indices and output an image with those pixels.</p>
<h3 id="quantize">quantize</h3>
<p>Quantizing the image according to the number of bits allotted we apply the following formula:</p>
<p><img src=".\Assignment1\kdrucia1_HTML\quantize.gif" alt=""></p>
<h3 id="randomdither">randomDither</h3>
<p>I generated a uniform random distribution in the range (-1, 1). I then add a bit of noise to each of the pixels according to the formula:</p>
<p><img src=".\Assignment1\kdrucia1_HTML\rDither.gif" alt=""></p>
<h3 id="ordereddither2x2">orderedDither2x2</h3>
<p>Ordered dither with a 2x2 matrix essentially decreases the resolution of the image 4 times. This method takes advantage of the fact that the human eye perceives groups of pixels as a single color. For example, given black and white, ordering the pixels in a a specific manner can yield different shades of gray.  The dithering matrix is:</p>
<p><img src=".\Assignment1\kdrucia1_HTML\oDither.gif" alt=""></p>
<p>I find the index into the matrix from the remainder of indices x and y from the original image. From there I find the difference of the bits-adjusted value of the pixel from its floor (c). If the index into the matrix divided by n^2 + 1 is greater than the error, the value at that pixel is the ceiling of c; else, floor of c.</p>
<h3 id="floydsteinbergdither">floydSteinbergDither</h3>
<p>Floyd-Steinberg dithering distributes the error of the quantized pixel to the next pixels in the 3x3 matrix surrounding the pixel according to this image:</p>
<p><img src=".\Assignment1\kdrucia1_HTML\fsDither.png" alt=""></p>
<p>The error calculated in Floyd-Steinberg dithering is the difference between the value of the original pixel and quantized pixel. </p>
<h3 id="blur3x3">blur3x3</h3>
<p>In blur3x3, we apply a gaussian blur to the entire image according to the normalized mask:</p>
<p><img src=".\Assignment1\kdrucia1_HTML\blur3x3.gif" alt=""></p>
<p>Every pixel has their values average by the values of the surrounding pixels weighted according to the mask.</p>
<h3 id="edgedetect3x3">edgeDetect3x3</h3>
<p>In edgeDetect3x3, we apply another mask that calculates error between pixels. </p>
<p><img src=".\Assignment1\kdrucia1_HTML\edges3x3.gif" alt=""></p>
<p>If the values of the neighboring pixels are similar to the value of the pixel at hand, the result of applying this matrix will be close to zero. The further from zero, the more that the pixel differs from its neighbors and is thus an edge. In my implementation, I determined an edge to be a pixel whose error surpasses an arbitrary threshold. In this case, I chose 20. This error is calculated for each color channel. Therefore, for pixels differing from their neighbors in each color channel, the output pixel is white. If only the red channel differs, the output pixel is red, etc. (Method 1)</p>
<p>It is worth noting that there are multiple method of displaying the edge of an image. Another option I considered was to iterate over the image once and find the max errors for each channel. Thus, on my second iteration, I output a color corresponding to how much the pixels error defers from the max error of that pixel. (Method 2)</p>
<h5 id="method-1-">Method 1:</h5>
<p><img src=".\Assignment1\kdrucia1_HTML\EdgeDetection.bmp" alt="Method 1"></p>
<h5 id="method-2-">Method 2:</h5>
<p><img src=".\Assignment1\kdrucia1_HTML\EdgeDetection2.bmp" alt="Method 2"></p>
<h3 id="nearestsample">nearestSample</h3>
<p>This method samples the image according to the floor of the given pixel + 0.5.</p>
<h3 id="bilinearsample">bilinearSample</h3>
<p>This method finds the four existing pixel values surrounding the given point and takes their weighted average. </p>
<h3 id="gaussiansample">gaussianSample</h3>
<p>This method calculates the weighted average of the given pixel according to a gaussian distribution. The pixels sampled for the gaussian distribution are those that fall within some radius of the given pixel. The weights of the pixels considered with the function:</p>
<p><img src=".\Assignment1\kdrucia1_HTML\gaussian.gif" alt="Method 2"></p>
<p>where d is the distance between a pixel is the radius and the center pixel. These weights are normalized and the color of the pixel is obtained.</p>
<h3 id="scalenearest">scaleNearest</h3>
<p>Create blank image with the size of the final image. Iterate over the blank image and apply nearestSample to every pixel in the source image (reverse mapping). If the sampled pixel is out of bounds, set to 0. </p>
<h3 id="scalebilinear">scaleBilinear</h3>
<p>Create blank image with the size of the final image. Iterate over the blank image and apply bilinearSample to every pixel (reverse mapping). If any of the four pixel being sampled are out of bounds, set to 0.</p>
<h3 id="scalegaussian">scaleGaussian</h3>
<p>Create blank image with the size of the final image. I chose the radius to be 1.0 / scaleFactor. If the radius is not at least 1, increment it. I iterate over the blank image and then take a gaussian sample of the source image with variance = radius / 3.</p>
<h3 id="rotatenearest">rotateNearest</h3>
<p>As a note for all rotation methods, I accidently coded a clockwise rotation. Since the example output displayed a counter clockwise rotation, I just negated the angle at the start of each method. </p>
<p>In order to display the entirety of the original image after a rotation, the original image size is changed. This new size is calculated at the start to be:</p>
<p><img src=".\Assignment1\kdrucia1_HTML\newHeight.gif" alt=""></p>
<p><img src=".\Assignment1\kdrucia1_HTML\newWidth.gif" alt=""></p>
<p>Next, I apply the rotation to the index of the pixel and obtain the coordinates (u, v). The source image is then sampled using nearestSampling to obtain the final rotated image. Any pixels that are out of bounds in the source image are set to 0. This way, the final image appears in its entirety as if its overlaid on a black background. </p>
<h3 id="rotatebilinear">rotateBilinear</h3>
<p>I first calculate the new height and width and then iterate over the blank final image. For each pixel in the final image, I apply bilinearSampling to the source image in order to obtain the final image. </p>
<h3 id="rotategaussian">rotateGaussian</h3>
<p>I first calculate the new height and width and the iterate over the blank final image. For each pixel in the final image, I apply gaussianSampling to the source image in order to obtain the final image. </p>
<h3 id="setalpha">setAlpha</h3>
<p>This method simply reads the red channel of an input image and writes them to another image&#39;s alpha channels. Since jpg and bmp are 3-channel image formats, this allows us to manually simulate an alpha channel in the composite method. </p>
<h3 id="composite">composite</h3>
<p>Composite takes in the overlay image with its alpha channels populated by setAlpha and convolves it with some source image. I first make sure that the two images are the same size. I then merge the images by the following formula:</p>
<p><img src=".\Assignment1\kdrucia1_HTML\composite.gif" alt=""></p>
<p>If a pixel does not have an alpha value, I set the alpha value at that pixel to 1.0 (255).</p>
<h3 id="funfilter">funFilter</h3>
<p>My fun filter creates the effect of an oil painting.</p>
<p>The algorithm for the oil painting effect is actually quite simple. I iterate over the image and create a numBuckets sized array for each of the color channels and a numBuckets sized array to store the frequency of the different luminance values which gets populated for each of the pixels. I then iterate over some radius around each pixel and find its luminance normalizing that value so that it falls into one of the numBuckets buckets. I then increment the frequency array at that luminance&#39;s index. Finally, I iterate over the buckets and find the most frequently occurring luminance value. The final pixel&#39;s color becomes the value in the bucket of most frequently occurring luminance value divided by how many pixels compose the total value.  Since neighboring pixels have similar luminance values, this gives a blotchy image similar to an oil painting.</p>
<p>Increasing the radius is computationally expensive, but makes the image blotchier like a rough oil painting. Increasing the number of buckets also makes the image blotchier. I found by testing many values that the most optimal oil painting effect is achieved through a bucket size of about 20 and a radius of about 7. Increasing the bucket size too much makes the image too noisy. Increasing the radius too much compromises fine details. Of course these numbers are image-size specific. These are what I found to work for a 1080x1080 sized image. </p>
<h2 id="beier-neely-morphing-">Beier-Neely Morphing:</h2>
<h3 id="warp">warp</h3>
<p>For this  method, I simply implemented the Beier-Neely morphing algorithm as outlined in the research paper. The only difference I suppose is when sampling the original. I decided to use bilinear sampling. Linear sampling results in jagged edges and Gaussian sampling is too computationally expensive. This was an important choice as I decided to create a 400-frame gif and one of my outputs. </p>
<p>I opted not to fix the &quot;out of bounds&quot; pixels. As a result the output frames have artifacts where I try to sample outside of the range of the source image.</p>
<h3 id="crossdissolve">CrossDissolve</h3>
<p>Here I just added the difference in color between image 1 and image 2 multiplied by some blending weight to the source image for every pixel. The dimensions of the new image is the dimensions of image 2 (destination image).</p>
<h3 id="length">length</h3>
<p>This is simply the distance formula applied to the end points of a line segment.</p>
<h3 id="distance">distance</h3>
<p>I had a rather hard time implementing this function. I could not figure out how to calculate the distance of the point to the line without recalculating u and v as specified in the research paper. Although I could have just included implementation of the distance method in the body of the warp method, I opted to recalculate u and v anyway in the distance method. </p>
<p><img src=".\Assignment1\kdrucia1_HTML\distance.png" alt=""></p>
<h3 id="perpendicular">perpendicular</h3>
<p>I found a clever hack for this method where I create a new point by subtracting the second endpoint from the first and then returning a new point as (-second, first).</p>
<h3 id="getsourceposition">GetSourcePosition</h3>
<p>I did not implement this method. I assume this is the bulk of the warping method, but I opted to just implement that in the actual warp method itself.</p>
<h2 id="notes-on-output-">Notes on output:</h2>
<h4 id="movie-">movie/art contest: </h4>
<p><img src=".\Assignment1\kdrucia1_HTML\kdrucia1.art.101.gif" alt=""></p>

<p><strong>In order to generate the .gif movie I had to change code in the main method</strong> that calls the morph method. <strong>The line is question is line 132 of main1.cpp</strong>.  I wrote a script that creates all of the frames for the movie, but I could not figure out how to pass a floating point number to the method from the script (bash doesn&#39;t natively support it). As a workaround, I just divide the whole numbered threshold by the number of frames in the output gif minus 1 to get the threshold for that image. </p>
<p>The command used to create the movie is:</p>
<p>.\createseries.sh --in ./Images/new --bnMorph ./Images/new ./Lines/ --out ./newOut/</p>
<p>This is run from within the &quot;evolution_of_man&quot; folder.</p>
<ul>
<li><strong>--in</strong>: passes the string &quot;--in&quot; to the script</li>
<li><strong>./Images/new</strong>: passes the first part of the file containing the source image. I iterate from 1 to 16 and append the number as well as the filetype to the end of &quot;./Images/new&quot; to generate something like &quot;./Images/new14.jpg&quot;</li>
<li><strong>--bnMorph</strong>: passes the string &quot;--bnMorph&quot; to the script</li>
<li><strong>./Images/new</strong>: passes the first part of the file containing the destination image. I iterate from 2 to 17 and append the number as well as the filetype to the end of &quot;./Images/new&quot;.</li>
<li><strong>./Lines/</strong>: passes the lines folder to the script. Inside this folder is are files named &quot;1.txt&quot; through &quot;16.txt&quot;. These contains the line segment information for the morphs. </li>
<li><strong>--out</strong>: passes the string &quot;--out&quot; to the script.</li>
<li><strong>./newOut</strong>: passes the folder containing all of the output transformations. For the evolution of man gif, 400 images are generated and labeled from &quot;1.jpg&quot; to &quot;400.jpg&quot;</li>
</ul>
<p>The script doesn&#39;t actually create the gif. It just create the 400 images. I stitched these together using <a href="https://ezgif.com/maker">https://ezgif.com/maker</a>.</p>
<h4 id="10-frames-">10 frames:</h4>
<p>I used a different script to create the 10 frames of me morphing into professional soccer player, Neymar. </p>
<p>The command used to create the movie is:</p>
<p>.\createseries.sh --in .\me.jpg --bnMorph .\neymar.jpg .\me_to_neymar_lines.txt --out .\me_to_neymar_frames\out</p>
<p>This is run from within the &quot;me_to_neymar&quot; folder.</p>
<h2 id="additional-notes-">Additional Notes:</h2>
<p>While trying to develop a cool fun filter, I ended up writing a few methods which I never came to use. They are integrated with the existing command line parser and as such can be run on images from the command line.</p>
<h4 id="blurnxn">blurNXN</h4>
<p>This is run with the name --blurNXN and takes in a sigma and a variance. It does essentially what blur3x3 does but scaled to any radius and any sigma. </p>
<h4 id="shiftchannel">shiftChannel</h4>
<p>This is run with the name --shiftChannel. The user specifies the desired channel and amount to shift it by. This method just add the specified amount to the specified channel for every pixel in the image. The channels are labeled with integers (0 for alpha, 1 for red, 2 for green, 3 for blue).</p>


